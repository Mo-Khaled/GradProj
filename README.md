# ğŸš— AI-Powered Obstacle Detection & Monitoring System for Smart Cars

## ğŸ” Overview

This project is an end-to-end AI agent system that uses LiDAR point cloud data to:
- Detect obstacles in real-time.
- Assess obstacle severity.
- Initiate safe actions: from alerts to autonomous braking.
- Provide driver assistance via PointNet visualization.
- Monitor internal car health and display it on a car-friendly UI.
- Simulate the full environment using open-source tools.

This solution is built on **Raspberry Pi**, and integrates **LiDAR**, **AI models (like PointNet)**, and a **React-based UI** for real-time data visualization.

---

## ğŸ‘¥ Team Roles

| Member | Role | Responsibilities |
|--------|------|------------------|
| Member 1 | Hardware Integration Engineer | Configure Raspberry Pi, interface with LiDAR, setup CAN Bus, GPIO sensors. Tools: Raspberry Pi OS, Python, SSH. |
| Member 2 | Data Engineer | Collect, preprocess and annotate point cloud data, optimize dataset pipeline. Tools: Python, Open3D, ROS, pandas. |
| Member 3 | AI/ML Engineer | Implement PointNet for classification/segmentation, train/test/validate obstacle detection model. Tools: PyTorch, CUDA, matplotlib, NumPy. |
| Member 4 | Simulation & Testing | Run Carla/LGSVL/ROS Gazebo simulations, test ADAS responses, log metrics. Tools: Carla, ROS2, Python. |
| Member 5 | UI & Integration Engineer | Build car dashboard interface to show detected objects, alerts, and internal sensors. Tools: React.js, Flask, WebSocket, CAN Bus SDKs. |

---

## ğŸ› ï¸ Technologies & Tools

- **Hardware**: Raspberry Pi 4, LiDAR sensor (Velodyne or RPLIDAR), CAN Bus adapter
- **Software**: Python, PyTorch, ROS2, Open3D, Carla/LGSVL, Flask, React.js
- **AI Models**: PointNet (Segmentation + Classification)
- **DevOps & Deployment**: Docker, Dify.ai (for LLM agents), GitHub Actions

---

## ğŸ§  AI Model Pipeline

1. **Input**: Raw 3D point cloud from LiDAR.
2. **Transform**: Through T-Net for alignment invariance.
3. **Feature Extraction**: Using shared MLP layers.
4. **Classification/Segmentation**: Obstacle detected, object class, relative location.
5. **Severity Analysis**: Calculate risk based on size, location, speed.
6. **Decision Making**: Either alert the driver or initiate braking.

---

## ğŸ—ºï¸ Environment Mapping & Obstacle Avoidance

- 3D environment built using SLAM or Octomap.
- Point clustering (DBSCAN) for object separation.
- YOLO (optional camera fusion) for visual cross-verification.
- Braking/Steering signals passed via GPIO or CAN Bus.

---

## ğŸ§ª Simulation & Testing

- **Tool**: Carla Simulator + ROS Bridge
- Test scenarios include:
  - Pedestrian crossing unexpectedly.
  - Static obstacle detection.
  - Night-time & low-light obstacle visibility.
- Evaluate false positives, false negatives, braking delay.

---

## ğŸ–¥ï¸ Driver Dashboard UI

- Built using React.js & WebSocket connection to Raspberry Pi.
- Displays:
  - Real-time segmented LiDAR view (PointNet Output)
  - Oil temperature, tire pressure, internal system diagnostics.
  - Alerts for detected objects.
  - Current driving suggestion (Manual / Auto Brake)

---

## ğŸ§± GitHub Repository Structure

```
ai-car-agent/
â”‚
â”œâ”€â”€ backend/                  # Flask + AI models
â”‚   â”œâ”€â”€ lidar_reader.py
â”‚   â”œâ”€â”€ pointnet_model.py
â”‚   â”œâ”€â”€ severity_analyzer.py
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                 # React.js dashboard
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/
â”‚       â””â”€â”€ App.js
â”‚
â”œâ”€â”€ simulation/               # Carla/ROS2 scenarios
â”‚   â””â”€â”€ pedestrian_test.launch
â”‚
â”œâ”€â”€ deployment/               # Docker, Dify, agent logic
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/                     # Preprocessed LiDAR datasets
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ labels/
â”‚
â”œâ”€â”€ docs/                     # Design documents & deliverables
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“¦ Deployment

- Package backend using **Docker** for Raspberry Pi ARM build.
- Use **Dify** for natural language interface (optional):
  - Example: â€œIs it safe to overtake now?â€ â†’ Dify consults vision + speed.
- Auto start agent on boot using `systemd` service.
- Use **RAG (Retrieval-Augmented Generation)** to reduce hallucination if LLMs involved (e.g., querying manuals for sensor errors).

---

## ğŸ“š Sample Datasets & Simulation Resources

- [SemanticKITTI](http://www.semantic-kitti.org/dataset.html) â€“ Point cloud semantic segmentation
- [Carla Simulator Scenarios](https://carla.org)
- [Open3D LiDAR examples](http://www.open3d.org/)
- [Velodyne sample data](https://github.com/VelodyneLiDAR)

---

## âœ… Deliverables

- [x] Preprocessed LiDAR Dataset
- [x] Trained PointNet Model
- [x] Raspberry Pi LiDAR Stream
- [x] React.js Dashboard
- [x] Simulation Logs & Metrics
- [x] GitHub Repo with Docker Deployment
- [x] Final Report + Architecture Diagram

---
